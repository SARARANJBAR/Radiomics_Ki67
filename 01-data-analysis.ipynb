{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Data Analysis\n",
    "\n",
    "In this notebooks, we will go through the implementation of each of the steps in the Machine Learning Pipeline. \n",
    "\n",
    "We will discuss:\n",
    "\n",
    "1. **Data Analysis**\n",
    "2. **Feature Engineering**\n",
    "3. Feature Selection\n",
    "4. Model Training\n",
    "5. Obtaining Predictions / Scoring\n",
    "\n",
    "# Project Context\n",
    "\n",
    "This radiomics experiment is focused on connecting tissue properties with imaging patterns. The data originated from two MRI-localized biopsy cohorts. GBM patients were conscented and enrolled in Columbia and Mayo Clinic biopsy collection programs where multiple image localized biopsies were extracted from their tumor prior to gross or subtotal resection. Samples were sent to pathology for tissue analysis. In this work we focus on Ki67, a marker of cell proliferation. On the other side we have coregistered imaging data associated with the same time point as tissue extraction, including qualitative MRI sequences such as T1-weighted post contrast injection (T1Gd), T2-weighted (T2), and a quantitative MRI, apparent diffusion coefficients (ADC). \n",
    "\n",
    "Each of these image type were preprocessed offline, appropriate for the type of MRI that they belonged to, and quantitative features were extracted from a small area around each biopsy location using the pyradiomics pipeline.\n",
    "The result of this analysis is a input csv file where each row data related to a unique biopsy, including  pyradiomics imaging features from 3 MRI types as well as the target, and some potentially biologically relevant features such as patient sex, age at death if available, type of tumor: recurrent or primary, and the source institution.\n",
    "\n",
    "===================================================================================================\n",
    "\n",
    "## Predicting Ki67 abbundance\n",
    "\n",
    "The aim of the project is to build a machine learning model to predict the abbundance of ki67 in biopsies based on different imaging features describing patterns around the biopsies.\n",
    "\n",
    "\n",
    "### Why is this important? \n",
    "\n",
    "Predicting ki67 is useful to identify if imaging patterns explain proliferation is biospy samples. We know that proliferation is elevated where tumor cells are present. If we can predict where proliferation happens we can basically create maps corresponding to the spatial distribution of tumor cells across whole tumors. So basically identifying where tumor cells are. These maps can theoretically inform radiation plans,, improving the efficacy of radiation therapy.\n",
    "\n",
    "\n",
    "### What is the objective of the machine learning model?\n",
    "\n",
    "We aim to minimise the difference between the real and the estimated abbundance of the target by our model. We will evaluate model performance with the:\n",
    "\n",
    "1. mean squared error (mse)\n",
    "2. root squared of the mean squared error (rmse)\n",
    "3. r-squared (r2).\n",
    "\n",
    "\n",
    "### How do I download the dataset?\n",
    "\n",
    "you cant. it is proprietary data, sorry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Let's load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle paths\n",
    "import os\n",
    "\n",
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for the yeo-johnson transformation\n",
    "import scipy.stats as stats\n",
    "\n",
    "# to display all the columns of the dataframe in the notebook\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318, 5251)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "datadir = os.path.join(os.getcwd(), 'dataset')\n",
    "data = pd.read_csv(os.path.join(datadir, 'cumc+csbc_t1gd+t2+adc_predKi67ForMayo_pyradiomics.csv'))\n",
    "\n",
    "# rows and columns of the data\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Anon id, it is just a number given to identify each sample\n",
    "# source too, it is just the institution name\n",
    "dropcols = ['AnonID', 'Source']\n",
    "for c in dropcols:\n",
    "    data.drop(c, axis=1, inplace=True)\n",
    "\n",
    "# set index to the unique biopsy id for each row\n",
    "data.set_index('biopsyImage', inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 318 rows, that is, biopsy samples, and 5248 columns, i.e., variables. \n",
    "\n",
    "Some will be predictive, a lot are going to be redundant, some are sample descriptive, and one is the target variable: Ki67 LI\n",
    "\n",
    "## Analysis\n",
    "\n",
    "**We will analyse the following:**\n",
    "\n",
    "1. The target variable\n",
    "2. Variable types (categorical and numerical)\n",
    "3. Missing data\n",
    "4. Numerical variables\n",
    "    - Discrete\n",
    "    - Continuous\n",
    "    - Distributions\n",
    "    - Transformations\n",
    "\n",
    "5. Categorical variables\n",
    "    - Special mappings\n",
    "    \n",
    "\n",
    "## Target\n",
    "\n",
    "Let's begin by exploring the target distribution. First lets check there are no missing values in target, if there is drop the row altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Ki67 LI'\n",
    "if data[target].isnull().sum() == 0:\n",
    "    print('no missing target')\n",
    "else:\n",
    "    df = df.dropna(subset=[target])\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram to evaluate target distribution\n",
    "data[target].hist(bins=50, density=True)\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel(target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target is continuous, and the distribution is skewed towards the left. We have one extreme outlier also, which is going to complicate things. But dropping samples is never a good idea, specially in a case like this where samples are scarse.\n",
    "\n",
    "Lets see if we can improve the spread of target with a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the target using the logarithm is not possible\n",
    "# since there are 0s in the target. \n",
    "# we will use yeo-johnson instead\n",
    "target_tr_vals, _ = stats.yeojohnson(data[target])\n",
    "target_tr_vals = list(target_tr_vals)\n",
    "#np.log(data[target]).hist(bins=50, density=True)\n",
    "plt.hist(target_tr_vals, bins=50, density=True)\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('%s - transformed' % target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "much better.\n",
    "\n",
    "## Variable Types\n",
    "\n",
    "Next, let's identify the categorical and numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's identify the categorical variables\n",
    "# we will capture those of type *object*\n",
    "\n",
    "cat_vars = [var for var in data.columns if data[var].dtype == 'O']\n",
    "print(len(cat_vars))\n",
    "print(cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast all cat variables as categorical\n",
    "data[cat_vars] = data[cat_vars].astype('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's identify the numerical variables\n",
    "num_vars = [\n",
    "    var for var in data.columns if var not in cat_vars and var != target\n",
    "]\n",
    "\n",
    "# number of numerical variables\n",
    "len(num_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok.\n",
    "\n",
    "# Missing values\n",
    "\n",
    "Let's go ahead and find out which variables of the dataset contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the variables that contain missing values\n",
    "vars_with_na = [var for var in data.columns if var!=target and data[var].isnull().sum() > 0]\n",
    "print('%d variables with missing values' % len(vars_with_na))\n",
    "# determine percentage of missing values (expressed as decimals)\n",
    "# and display the result ordered by % of missin data\n",
    "\n",
    "data[vars_with_na].isnull().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains one variables with a big proportion of missing values (Age at Dealth). And a whole lot of other variables with a small percentage of missing observations.\n",
    "\n",
    "This means that to train a machine learning model with this data set, we need to impute the missing data in these variables.\n",
    "\n",
    "We can also visualize the percentage of missing values in the variables as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ploting cell : \n",
    "### dont plot though, 5000 vars are alot\n",
    "\n",
    "data[vars_with_na].isnull().mean().sort_values(\n",
    "    ascending=False).plot.bar(figsize=(10, 4))\n",
    "plt.ylabel('Percentage of missing data')\n",
    "plt.axhline(y=0.50, color='r', linestyle='-')\n",
    "plt.axhline(y=0.10, color='g', linestyle='-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can determine which variables, from those with missing data,\n",
    "# are numerical and which are categorical\n",
    "\n",
    "cat_na = [var for var in cat_vars if var in vars_with_na]\n",
    "num_na = [var for var in num_vars if var in vars_with_na]\n",
    "\n",
    "\n",
    "print('Number of categorical variables with na: ', len(cat_na))\n",
    "print('Number of numerical variables with na: ', len(num_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between missing data and Ki67\n",
    "\n",
    "Let's evaluate the target in samples where the information is missing. We will do this for each variable that shows missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_na_value(df, var):\n",
    "\n",
    "    # copy of the dataframe, so that we do not override the original data\n",
    "    # see the link for more details about pandas.copy()\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html\n",
    "    df = df.copy()\n",
    "\n",
    "    # let's make an interim variable that indicates 1 if the\n",
    "    # observation was missing or 0 otherwise\n",
    "    df[var] = np.where(df[var].isnull(), 1, 0)\n",
    "\n",
    "    # let's compare the median SalePrice in the observations where data is missing\n",
    "    # vs the observations where data is available\n",
    "\n",
    "    # determine the median abundance of target in the groups 1 and 0,\n",
    "    # and the standard deviation of the target,\n",
    "    # and we capture the results in a temporary dataset\n",
    "    tmp = df.groupby(var)[target].agg(['mean', 'std'])\n",
    "\n",
    "    # plot into a bar graph\n",
    "    tmp.plot(kind=\"barh\", y=\"mean\", legend=False,\n",
    "             xerr=\"std\", title=target, color='green')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's run the function on each variable with missing data\n",
    "for var in num_na[:5]:\n",
    "    analyse_na_value(data, var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like in numeric variables, the average target in samples with missing is lowerfrom cases with the variable is present. This could suggest that data being missing could be a good predictor of target. However, confidence intervals overlap though, so take this with a grain of salt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal variables\n",
    "\n",
    "We dont have temporal variables in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete variables\n",
    "\n",
    "Let's go ahead and find which variables are discrete, i.e., show a finite number of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  let's male a list of discrete variables\n",
    "discrete_vars = [var for var in num_vars if len(\n",
    "    data[var].unique()) < 20]\n",
    "\n",
    "print('Number of discrete variables: ', len(discrete_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Ok. no discrete variables. move on to categorical variables.\n",
    "\n",
    "## Categorical variables\n",
    "\n",
    "how many unique values do we have in these categorical values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[cat_vars].nunique().sort_values(ascending=False).plot.bar(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like we only have 2 values per variable. great. lets map them to numeric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in cat_vars:\n",
    "    # make boxplot with Catplot\n",
    "    sns.catplot(x=var, y=target, data=data, kind=\"box\", height=4, aspect=1.5)\n",
    "    # add data points to boxplot with stripplot\n",
    "    sns.stripplot(x=var, y=target, data=data, jitter=0.1, alpha=0.3, color='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for some categorical variables we dont see a marked difference: sex for instance. \n",
    "Whereas for the type of tumor (primary or recurrent) we see higher ki67 values for primary cases. This makes sense biologically. \n",
    "\n",
    "Transform categorical variables into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is alphabetical order for which will be 0 and which will be 1.\n",
    "cat_mappings = {'Sex':{'F':0, 'M': 1}, 'Primary/Recurrent':{'Primary':0, 'Recurrent': 1}}\n",
    "\n",
    "# map the variable values\n",
    "for var in cat_vars:\n",
    "    var_map = cat_mappings[var]\n",
    "    data[var] = data[var].map(var_map)\n",
    "\n",
    "data[cat_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numeric variables\n",
    "\n",
    "Let's go ahead and find the distribution of the numeric (continuous in this project). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualise the numeric variables\n",
    "data[num_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting 5000 variables is ridiculous, we need to create a test that captures skewness quantitatively.\n",
    "Statistically, two numerical measures of shape – skewness and excess kurtosis – can be used to test for normality. If skewness is not close to zero, then your data set is not normally distributed.\n",
    "\n",
    "Skewness is a measure of the asymmetry of the probability distribution of a random variable about its mean. In other words, skewness tells you the amount and direction of skew (departure from horizontal symmetry). The skewness value can be positive or negative, or even undefined. If skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. As a general rule of thumb:\n",
    "\n",
    "skewness < -1 or skewness > 1 ----> highly skewed.\n",
    "\n",
    "-1 <skewness < -0.5 | 0.5 < skewness < 1, ----> moderately skewed.\n",
    "\n",
    "-0.5 < skewness < 0.5 ----> approximately symmetric. or no skewness\n",
    "\n",
    "Lets find out how many of each we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on skewness\n",
    "def get_skewed_variables(df, varlist):\n",
    "    \n",
    "    skewness = df[varlist].skew(axis=0)\n",
    "    \n",
    "    moderately_skewed = []\n",
    "    threshold = 0.5\n",
    "    for var in varlist:\n",
    "        if 0.5 <= abs(skewness[var]) <= 1.0:\n",
    "            moderately_skewed.append(var)\n",
    "    \n",
    "    extremely_skewed = []    \n",
    "    threshold = 1  \n",
    "    for var in varlist:\n",
    "        if abs(skewness[var]) > 1.0:\n",
    "            extremely_skewed.append(var)\n",
    "    \n",
    "    return moderately_skewed, extremely_skewed\n",
    "\n",
    "print('# total numeric vars:', len(num_vars))\n",
    "num_vars_modskewed, num_vars_extskewed = get_skewed_variables(data, num_vars)\n",
    "print('# extremely skewed variables:', len(num_vars_extskewed))\n",
    "print('# moderately skewed variables:', len(num_vars_modskewed))\n",
    "\n",
    "# capture the remaining continuous variables\n",
    "cont_vars = [v for v in num_vars if v not in num_vars_modskewed+num_vars_extskewed]\n",
    "print('# ok numeric variables:', len(cont_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets visualize some of the moderately skewed ones\n",
    "for var in num_vars_modskewed[:5]: \n",
    "    \n",
    "    data[var].hist(bins=50, density=True)\n",
    "    plt.ylabel('counts')\n",
    "    plt.xlabel(var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply the yeo-johnson transformation to moderately skewed variables. Sometimes, transforming the variables to improve the value spread, improves the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yeo-Johnson transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = data.copy()\n",
    "for var in num_vars_modskewed:\n",
    "    data_tr[var], param = stats.yeojohnson(data_tr[var])\n",
    "\n",
    "res1, res2 = get_skewed_variables(data_tr, num_vars_modskewed)\n",
    "notfixable = res1 + res2\n",
    "fixable = [v for v in num_vars_modskewed if v not in notfixable]\n",
    "\n",
    "print('# modskewed variables that are maybe fixable with yeo-johnson:', len(fixable))\n",
    "print('# modskewed variables that are not fixable:', len(notfixable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's plot the original or transformed variables for fixable ones\n",
    "# vs target, and see if there is a relationship\n",
    "\n",
    "# visualize for the first 5\n",
    "target_logvals = np.log(data[target].values)\n",
    "for var in fixable[:5]:\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # plot the original variable vs target    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(data[var], target_logvals)\n",
    "    plt.ylabel(target)\n",
    "    plt.xlabel('Original ' + var)\n",
    "\n",
    "    # plot transformed variable vs sale price\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(data_tr[var], target_logvals)\n",
    "    plt.ylabel(target)\n",
    "    plt.xlabel('Transformed ' + var)\n",
    "                \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, the transformations doesn't seems to improve the relationship.\n",
    "Let's try a different transformation. \n",
    " \n",
    " ### Logarithmic transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's go ahead and analyse the distributions of these variables\n",
    "# after applying a logarithmic transformation. \n",
    "# we can only do this on variables that dont have 0\n",
    "fixable = [v for v in fixable if 0 not in data[v].values]\n",
    "\n",
    "data_tr = data.copy()\n",
    "\n",
    "for var in fixable:\n",
    "    # transform the variable with logarithm\n",
    "    data_tr[var] = np.log(data[var].values)\n",
    "    \n",
    "# visualize for the first 5\n",
    "target_logvals = np.log(data[target])\n",
    "for var in fixable[:5]:\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # plot the original variable vs target    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(data[var], target_logvals)\n",
    "    plt.ylabel(target)\n",
    "    plt.xlabel('Original ' + var)\n",
    "\n",
    "    # plot transformed variable vs sale price\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(data_tr[var], target_logvals)\n",
    "    plt.ylabel(target)\n",
    "    plt.xlabel('Transformed ' + var)\n",
    "                \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    " doesn't look like log transform did much for these variables. We can decide later if we want to do a transformation or not. Note this whether transformation actually helps improve the predictive power remains to be seen. To determine if this is the case, we should train a model with the original values and one with the transformed values, and determine model performance, and feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super skewed variables\n",
    "\n",
    "These super skewed variables are not normally distributed even with transformation. It is unlikely that a transformation will help change the distribution of these variables dramatically.\n",
    "Let's transform them into binary variables and see how predictive they are. But first how can we transform them? lets plot some:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in num_vars_extskewed[:5]: \n",
    "    \n",
    "    data[var].hist(bins=50, density=True)\n",
    "    plt.ylabel('counts')\n",
    "    plt.xlabel(var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well, yeah. they are very skewed. we can use the median to binarize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in num_vars_extskewed[:5]:\n",
    "    \n",
    "    tmp = data.copy()\n",
    "    \n",
    "    # map the variable values into 0 and 1\n",
    "    tmp[var] = np.where(data[var]<=data[var].median(), 0, 1)\n",
    "    \n",
    "    # determine mean sale price in the mapped values\n",
    "    tmp = tmp.groupby(var)[target].agg(['mean', 'std'])\n",
    "\n",
    "    # plot into a bar graph\n",
    "    tmp.plot(kind=\"barh\", y=\"mean\", legend=False,\n",
    "             xerr=\"std\", title=target, color='green')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be not much of a difference in target in the mapped values and the confidence intervals overlap, so most likely this is not significant or predictive.\n",
    "\n",
    "\n",
    "## summarize all you'll take to next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "\n",
    "def drop_select_vars(df, cols):\n",
    "    \"\"\"Drop unrelated or leaky columns.\"\"\"\n",
    "    for c in cols:\n",
    "        if c in df.columns.values:  # ensure column exists in df\n",
    "            df.drop(c, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_target_var(df, target):\n",
    "    \"\"\"\n",
    "    Transforms target variable based on skewness\n",
    "    using either yeojohnson or log transform.\n",
    "    \n",
    "    Returns df with target col transformed.\n",
    "    \"\"\"\n",
    "\n",
    "    assert target in df.columns.values\n",
    "    \n",
    "    # most of the times there are 0s in target \n",
    "    # find out if that is the case\n",
    "    has_zero = df[target].isin([0]).any().any()\n",
    "    \n",
    "    # if has zero use yeojohnson\n",
    "    if has_zero:\n",
    "        transformedvals, _ = stats.yeojohnson(df[target])\n",
    "        df[target] = transformedvals\n",
    "        return df\n",
    "    \n",
    "    # else, use log\n",
    "    df[target] = np.log(df[target].values)\n",
    "    return df\n",
    "\n",
    "\n",
    "def categorize_vars_based_on_skewness(df, varlist):\n",
    "    \"\"\"\n",
    "    Breaks varlist into 3 sublists: \n",
    "    notskewed, moderately skewed, extremely skewed\n",
    "    \n",
    "    \n",
    "    Definition for these categories:\n",
    "    \n",
    "    -  skewness < -1 or skewness > 1 : highly skewed.\n",
    "    -  -1 < skewness < -0.5 or 0.5 < skewness < 1 : moderately skewed.\n",
    "    -  -0.5 < skewness < 0.5 : approximately symmetric or no skewness\n",
    "    \n",
    "    Returns these 3 sublists.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(varlist) > 0\n",
    "\n",
    "    skewness = df[varlist].skew(axis=0)\n",
    "\n",
    "    moderately_skewed = []\n",
    "    threshold = 0.5\n",
    "    for var in varlist:\n",
    "        if 0.5 <= abs(skewness[var]) <= 1.0:\n",
    "            moderately_skewed.append(var)\n",
    "\n",
    "    extremely_skewed = []    \n",
    "    threshold = 1  \n",
    "    for var in varlist:\n",
    "        if abs(skewness[var]) > 1.0:\n",
    "            extremely_skewed.append(var)\n",
    "\n",
    "    # catch all others\n",
    "    notskewed = [v for v in varlist if var not in extremely_skewed+moderately_skewed]\n",
    "\n",
    "    return notskewed, moderately_skewed, extremely_skewed\n",
    "\n",
    "\n",
    "def get_cat_vars(df):\n",
    "    return [var for var in df.columns.values if df[var].dtype == 'O']\n",
    "\n",
    "\n",
    "def get_numericvars_transformation_plan(df, varlist):\n",
    "    \"\"\"\n",
    "    Sequentially breaks varlist into bins\n",
    "    based on their skewness and test a number of strategies fixes their skewness.\n",
    "    \n",
    "    strategies include 'Yeojohnson', and 'Log' transform. if these two dont work, we\n",
    "    will move on with binarizing the remaining variables using their median.\n",
    "    \n",
    "    Returns transformation dataframe with index as varlist and values for \n",
    "    the column showing the decision for that column.\n",
    "    \"\"\" \n",
    "    \n",
    "    # ensure numeric variables are actually numeric\n",
    "    # check if there are categorical variables among varlist\n",
    "    # remove these from the varlist\n",
    "    cat_vars = get_cat_vars(df[varlist])\n",
    "    if cat_vars:\n",
    "        for c in cat_vars:\n",
    "            varlist.remove(c)\n",
    "        \n",
    "    # create result dataframe to keep track of decisions\n",
    "    res = pd.DataFrame(index=varlist)\n",
    "    res['transformType'] = None\n",
    "    \n",
    "    # test skewness in variables\n",
    "    groups = categorize_vars_based_on_skewness(df, varlist)\n",
    "    notskewed, modskewed, extskewed = groups\n",
    "    print('notskewed, modskewed, extskewed counts:', [len(g) for g in groups])\n",
    "    \n",
    "    # If notskewed, no need for transformation\n",
    "    res.loc[notskewed, 'transformType'] = 'Skip'\n",
    "    \n",
    "    # If moderately skewed, apply yeo transform\n",
    "    # see if this transform fixed skewness\n",
    "    print('focusing on modskewed..')\n",
    "    df_tr = df.copy()\n",
    "    for var in modskewed:\n",
    "        df_tr[var], _ = stats.yeojohnson(df_tr[var])\n",
    "    groups = categorize_vars_based_on_skewness(df_tr, modskewed)\n",
    "    fixable, no_change, worsened = groups\n",
    "    print('fixable, no_change, worsened counts after yeojohnnson:', [len(g) for g in groups])\n",
    "    \n",
    "    # keep fixable results\n",
    "    res.loc[fixable, 'transformType'] = 'YeoJohnson'\n",
    "    \n",
    "    # see if log transform helps no_change and worsened vars\n",
    "    # make sure to only apply to cases without 0s\n",
    "    print('focusing on no change and worsened ones (without 0)')\n",
    "    targetlist = no_change + worsened\n",
    "    targetlist = [v for v in targetlist if not df[v].isin([0]).any().any()]\n",
    "    for var in targetlist:\n",
    "        df_tr[var] = np.log(df_tr[var])\n",
    "    groups = categorize_vars_based_on_skewness(df, targetlist)\n",
    "    fixable, no_change, worsened = groups\n",
    "    print('fixable, no_change, worsened counts after yeojohnnson:', [len(g) for g in groups])\n",
    "\n",
    "    # update res for fixable\n",
    "    res.loc[fixable, 'transformType'] = 'Log'\n",
    "    \n",
    "    # all remaining have to be binarized\n",
    "    # you can use different criteria, we'll use medium here\n",
    "    # update varlist\n",
    "    print('everything else cant be fixed. binarizing  ')\n",
    "    binthese = [v for v in varlist if res.loc[v, 'transformType'] is None]\n",
    "    res.loc[binthese, 'transformType'] = 'Binarize'\n",
    "    \n",
    "    # you are done. return result\n",
    "    for v in ['Skip', 'YeoJohnson', 'Log', 'Binarize']:\n",
    "        print(v, res['transformType'].count(v))\n",
    "    return res\n",
    "\n",
    "\n",
    "def transform_cat_vars(df, trans_map):\n",
    "    \"\"\"\n",
    "    Given a transformmation plan, this function turns categorical vars \n",
    "    into numeric vars.\n",
    "    \"\"\"\n",
    "    # ensure trans_plan is not empty\n",
    "    assert type(trans_map) == dict\n",
    "    \n",
    "    if trans_map == {}:\n",
    "        print('no plan provided. return df unchanged')\n",
    "        return df\n",
    "    \n",
    "    for var in trans_map.keys():\n",
    "        \n",
    "        prevals = set(df[var].values)\n",
    "        postvals = set(trans_map[var].values())\n",
    "        \n",
    "        # check if pre and post vals have no overlap\n",
    "        if prevals.isdisjoint(postvals):\n",
    "            df[var] = df[var].map(trans_map[var])\n",
    "        else:\n",
    "            print('%s already transformed.skip.' % var)\n",
    "            # transform already done. dont do it again\n",
    "            continue\n",
    "        \n",
    "    # done. return transformed df\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_num_vars(df, trans_map):\n",
    "    \"\"\"\n",
    "    Given a transformmation plan, this function applies all the requested \n",
    "    transforms to the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ensure trans_plan is not empty\n",
    "    if trans_map.empty:\n",
    "        print('no plan provided. return df unchanged')\n",
    "        return df\n",
    "    \n",
    "    plancol = 'transformType'\n",
    "    uniq_plans = list(set(trans_map[plancol].values))\n",
    "    \n",
    "    df_tr = df.copy()\n",
    "    \n",
    "    for pl in uniq_plans:\n",
    "        pl_vars = trans_map[trans_map[plancol] == pl].index.values\n",
    "        \n",
    "        if pl == 'Skip':  # do nothing\n",
    "            pass\n",
    "        \n",
    "        elif pl == 'YeoJohnson':\n",
    "            for var in pl_vars:\n",
    "                df_tr[var], _ = stats.yeojohnson(df[var])\n",
    "        \n",
    "        elif pl == 'Log':\n",
    "            for var in pl_vars:\n",
    "                df_tr[var] = np.log(df[var])\n",
    "        \n",
    "        elif pl == 'Binarize':\n",
    "            for var in pl_vars:\n",
    "                df_tr[var] = np.where(df[var]<=df[var].median(), 0, 1)\n",
    "    \n",
    "        else:  # do nothing\n",
    "            print('transformation %s unknown. skip these' % pl)\n",
    "            pass\n",
    "    \n",
    "    # done. return transformed df\n",
    "    return df_tr\n",
    "\n",
    "\n",
    "def fill_in_missing_vals(df):\n",
    "    \n",
    "    # find vars with missing values\n",
    "    vars_na = [v for v in df.columns.values if df[v].isnull().sum()>0]\n",
    "    \n",
    "    # skip if no missing vals\n",
    "    if not vars_na:\n",
    "        return df\n",
    "    \n",
    "    # divide into cat and num variable\n",
    "    cat_na = [v for v in vars_na if df[v].dtype == 'O']\n",
    "    num_na = [v for v in vars_na if v not in cat_na]\n",
    "    print('# categorical variables with na: ', len(cat_na))\n",
    "    print('# numerical variables with na: ', len(num_na))\n",
    "    \n",
    "    # for cat vars: use 0.3 as a guide/threshold (this is heuristically-decided)\n",
    "    # >= 30% missing: create a new category\n",
    "    # < 30% missing: replace with most commmon category\n",
    "    cat_miss_th = 0.3\n",
    "    perc_miss_df = df[cat_na].isnull().mean().sort_values(ascending=False)\n",
    "    \n",
    "    for v in cat_na:\n",
    "        na_indices = list(df[df[v].isnull() == True].index.values)\n",
    "        \n",
    "        if perc_miss_df[v] >= cat_miss_th:\n",
    "            new_cat = 'Unknown'\n",
    "            df.loc[na_indices, v] = new_cat\n",
    "        else:\n",
    "            # replace with most common category\n",
    "            df.loc[na_indices, v] = df[v].mode()\n",
    "            \n",
    "    ## for numeric, use 0.5 as threshold\n",
    "    # >= 50% missing: drop\n",
    "    # < 30% missing: replace with median\n",
    "    num_miss_th = 0.5\n",
    "    perc_miss_df = df[num_na].isnull().mean().sort_values(ascending=False)\n",
    "    \n",
    "    for v in num_na:\n",
    "        na_indices = list(df[df[v].isnull() == True].index.values)\n",
    "        \n",
    "        if perc_miss_df[v] >= num_miss_th:\n",
    "            df.drop(v, inplace=True)\n",
    "        else:\n",
    "            # replace with most common category\n",
    "            df.loc[na_indices, v] = df[v].median()\n",
    "    \n",
    "    # return new df\n",
    "    return df\n",
    "\n",
    "\n",
    "## unittests\n",
    "def test_fill_missing(df):\n",
    "    \n",
    "    tmp = fill_in_missing_vals(df)\n",
    "    vars_na = [v for v in tmp.columns.values if tmp[v].isnull().sum()>0]\n",
    "    assert len(vars_na) == 0\n",
    "\n",
    "\n",
    "def test_drop_select_vars(df, drop_vars):\n",
    "    \n",
    "    tmp = drop_select_vars(df, drop_vars)\n",
    "    for c in drop_vars:\n",
    "        assert c not in tmp.columns.values\n",
    "\n",
    "def test_transform_vat_vars(df, mapping):\n",
    "\n",
    "    tmp = transform_cat_vars(df, mapping)\n",
    "    for var in mapping.keys():\n",
    "        \n",
    "        prevals  = set(df[var].values)\n",
    "        postvals = set(tmp[var].values)\n",
    "        assert prevals != postvals\n",
    "        assert prevals not in postvals\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.path.join(os.getcwd(), 'dataset')\n",
    "data = pd.read_csv(os.path.join(datadir, 'cumc+csbc_t1gd+t2+adc_predKi67ForMayo_pyradiomics.csv'))\n",
    "\n",
    "target = 'Ki67 LI'\n",
    "indexcol = 'biopsyImage'\n",
    "drop_vars = ['Source', 'AnonID']\n",
    "cat_vars = ['Sex', 'Primary/Recurrent']\n",
    "num_vars =  [var for var in data.columns if var not in cat_vars+drop_vars+[target, indexcol]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline size: (318, 5250)\n",
      "after cleaning: (318, 5248)\n",
      "# categorical variables with na:  0\n",
      "# numerical variables with na:  5245\n",
      "size after filling missing: (318, 5248)\n"
     ]
    }
   ],
   "source": [
    "if data.index.name is not indexcol:\n",
    "    data.set_index(indexcol, inplace=True)\n",
    "print('baseline size:', data.shape)\n",
    "\n",
    "# 1. deal with drop vars\n",
    "data = drop_select_vars(data, drop_vars)\n",
    "print('after cleaning:', data.shape)\n",
    "\n",
    "# 2. clean up\n",
    "data = fill_in_missing_vals(data)\n",
    "print('size after filling missing:', data.shape)\n",
    "\n",
    "## tests\n",
    "test_drop_select_vars(data, drop_vars)\n",
    "test_fill_missing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m110994/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "# 3. transform target if necessary\n",
    "data = transform_target_var(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notskewed, modskewed, extskewed counts: [0, 717, 3740]\n",
      "focusing on modskewed..\n",
      "fixable, no_change, worsened counts after yeojohnnson: [717, 5, 0]\n",
      "focusing on no change and worsened ones (without 0)\n",
      "fixable, no_change, worsened counts after yeojohnnson: [0, 5, 0]\n",
      "everything else cant be fixed. binarizing  \n",
      "       transformType\n",
      "count           5245\n",
      "unique             2\n",
      "top         Binarize\n",
      "freq            4528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m110994/miniconda3/lib/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex already transformed.skip.\n",
      "Primary/Recurrent already transformed.skip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Primary/Recurrent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biopsyImage</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x323-x20161115-351-239-42</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x096-x20140109-377-170-74</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x116-x20140409-141-160-83</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x117-x20140414-34-109-105</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x077-x20131010-82-138-55</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Sex  Primary/Recurrent\n",
       "biopsyImage                                      \n",
       "x323-x20161115-351-239-42    2                  2\n",
       "x096-x20140109-377-170-74    2                  2\n",
       "x116-x20140409-141-160-83    1                  2\n",
       "x117-x20140414-34-109-105    1                  2\n",
       "x077-x20131010-82-138-55     2                  2"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform numeric predictors\n",
    "num_mappings = get_numericvars_transformation_plan(data, num_vars)\n",
    "data = transform_num_vars(data, num_mappings)\n",
    "\n",
    "# transformm categorical predictors\n",
    "cat_mappings = {'Sex': {'Unknown': 0, 'F': 1, 'M': 2}, \n",
    "                'Primary/Recurrent': {'Unknown': 0, 'Primary':1, 'Recurrent': 2}}\n",
    "\n",
    "data = transform_cat_vars(data, cat_mappings)\n",
    "data[cat_mappings.keys()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "644.467px",
    "left": "0px",
    "right": "1324px",
    "top": "110.533px",
    "width": "266px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
